

[
  
  
    
    
      {
        "title": "Hello World",
        "excerpt": "This is my very first blog post. I haven’t written anything yet but I’m sure I have some great stories to tell.\n",
        "content": "This is my very first blog post. I haven’t written anything yet but I’m sure I have some great stories to tell.\n",
        "url": "/general/2018/08/22/hello-world/"
      },
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Blog",
    "excerpt": "\n",
    "content": "\n",
    "url": "/blog/"
  },
  
  {
    "title": "The rise and fall of subbreddits",
    "excerpt": "\n",
    "content": "Project Title: Buzz, Bad Buzz or Good Buzz?\n\nAbstract (≈150 words)\nThis project investigates the relationship between cross-subreddit interaction patterns and community growth on Reddit. We analyze whether frequent external interactions (both positive and negative) correlate with faster subreddit population growth, and whether negative interactions (conflicts) are more strongly associated with growth than positive ones. By examining subreddits with varying levels of external connectivity, we aim to understand if interaction intensity can predict community growth trajectories. This research is relevant for understanding online community dynamics, social media platform growth patterns, and the role of controversy in digital community formation. Our analysis will provide insights into whether “any publicity is good publicity” holds true for online communities, with implications for community management strategies and platform design.\n\nResearch Questions\n\n  RQ1: Does frequent external interaction (positive or negative) correlate with faster growth of a subreddit community?\n  RQ2: Are negative interactions (conflict) more strongly associated with subreddit growth than positive ones?\n  RQ3: Do subreddits with little external interaction grow differently compared to highly connected ones?\n  RQ4: Can we predict subreddit growth based on interaction patterns?\n\n\nDatasets\n\n  Primary dataset: Reddit Subreddit Hyperlink Network\n    \n      Files:\n        \n          soc-redditHyperlinks-title.tsv (352MB): Hyperlinks in post titles\n          soc-redditHyperlinks-body.tsv (304MB): Hyperlinks in post bodies\n        \n      \n      Source: Research project on how subreddits attack one another (project website)\n      Size: 858,490 hyperlinks across 55,863 unique subreddits\n      Timespan: January 2014 - April 2017 (2.5 years)\n      Format: Tab-separated (TSV) with columns:\n        \n          SOURCE_SUBREDDIT: Origin subreddit\n          TARGET_SUBREDDIT: Destination subreddit\n          POST_ID: Post identifier\n          TIMESTAMP: Post timestamp\n          LINK_SENTIMENT: -1 (negative) or +1 (positive/neutral)\n          PROPERTIES: 86-dimensional text feature vector\n        \n      \n      Access: Files stored in data/raw/ directory\n      Key Features: Directed, signed (sentiment), temporal, attributed network\n    \n  \n  Additional dataset(s) (if any):\n    \n      Subreddit embeddings dataset (51,278 embeddings) - available separately\n        \n          Rationale: Can be used for similarity analysis and community detection\n          Note: Some subreddits lack embeddings, so coverage is ~92% of nodes\n        \n      \n      (WIP) Custom database scraped from Internet Archive snapshots of subreddit pages between January 2014 - April 2017 (2.5 years)\n    \n  \n\n\nData Handling Plan\n\n  Ingestion:\n    \n      TSV files loaded via src/data/loader.py::load_reddit_hyperlink_network()\n      Both title and body hyperlinks combined (deduplicated by SOURCE-TARGET-POST_ID)\n      Timestamps converted to datetime format\n    \n  \n  Preprocessing:\n    \n      Aggregate edge-level data to subreddit-level metrics:\n        \n          Incoming links (visibility/popularity metric)\n          Outgoing links (external engagement)\n          Positive/negative sentiment breakdowns\n        \n      \n      Calculate growth proxies:\n        \n          Since we lack consistent subscriber counts, use incoming link activity over time as proxy\n          Growth rate = ln(N_t/N_0) / t, where N is incoming links per time period\n        \n      \n      Filter subreddits with sufficient data (≥100 interactions) for meaningful analysis\n    \n  \n  Sampling/Filtering:\n    \n      Focus on subreddits with ≥100 total interactions for statistical reliability\n      Temporal filtering: Analyze monthly aggregations for growth trends\n    \n  \n  Scraping and Collection:\n    \n      Identify subreddits with enough valid snapshots captured on Internet Archive’s Wayback Machine and retrieve the snapshot timestamps\n      Scrape archived subreddit sites for accurate reader counts for each valid snapshot to complement growth proxies\n      Gather growth data in a new personal database for further analysis\n    \n  \n  Validation:\n    \n      Check for temporal consistency (links span Jan 2014 - Apr 2017)\n      Validate sentiment labels (-1 or +1)\n      Detect outliers in growth rates and interaction counts\n      Verify subreddit name consistency across source/target columns\n    \n  \n\n\nDescriptive Statistics and EDA\n\n  Network-level statistics:\n    \n      Total hyperlinks, unique subreddits, temporal coverage\n      Sentiment distribution (positive vs negative links)\n      Interaction frequency distributions (incoming/outgoing)\n    \n  \n  Subreddit-level aggregations:\n    \n      Total interactions per subreddit (incoming + outgoing)\n      Sentiment ratios (positive/negative breakdowns)\n      Growth rates based on incoming link activity over time\n      Growth rates based on archived reader counts\n    \n  \n  Correlation analysis:\n    \n      Interaction intensity vs growth rate\n      Positive vs negative interactions vs growth\n      Incoming vs outgoing links vs growth\n    \n  \n  Temporal patterns:\n    \n      Monthly aggregation of link counts and sentiment\n      Growth trends over the 2.5-year period\n      Seasonal patterns or trends in community interactions\n    \n  \n  Network characteristics:\n    \n      Degree distributions (in-degree/out-degree)\n      Communities with high/low external connectivity\n      Potential biases: sampling issues, popular subreddits dominating\n    \n  \n\n\nMethods (and essential mathematical details)\n\n  Growth Rate Scraping\n    \n      Use Wayback Machine’s CDX API to retrieve monthly CDX records for all studied subreddits, filtering out those that don’t have enough months recorded (threshold of $\\frac{3}{5}$ of total months).\n      Handle common network errors when retrieving CDX records by retrying until $&lt;5%$ of requests have errors.\n      For all valid subreddits (with enough monthly snapshots), scrape the archived sites’ HTML with requests + beautifulsoup4 libraries to retrieve reader counts.\n      Store all data in a new database for easy and fast access, as this scraping is very slow.\n    \n  \n  Growth Rate Proxy Calculation (proxy using incoming links):\n    \n      Since subscriber counts are unavailable in the given database, use incoming link activity: $r = \\frac{\\ln(N_t/N_0)}{t}$\n      Where $N_t$ is incoming links per time period (monthly aggregation)\n      Rationale: Incoming links indicate visibility/popularity, which correlates with community growth\n    \n  \n  Interaction Intensity Metrics:\n    \n      Total interactions: $\\sum_{i} I_i$ where $I_i$ is interaction count per subreddit\n      Sentiment-weighted interactions: $\\sum_{i} w_i \\cdot I_i$ where $w_i \\in {-1, 1}$ for negative/positive\n      Incoming vs outgoing: Separate metrics for visibility (incoming) vs engagement (outgoing)\n    \n  \n  Regression Analysis:\n    \n      Linear regression: $growth_rate = \\beta_0 + \\beta_1 \\cdot total_interactions + \\beta_2 \\cdot sentiment_ratio + \\epsilon$\n      Compare positive vs negative interactions: separate coefficients for $I_{pos}$ and $I_{neg}$\n    \n  \n  Comparative Analysis:\n    \n      ANOVA/t-test to compare growth rates across interaction intensity groups (low/medium/high)\n      Hypothesis: $H_0: \\mu_{low} = \\mu_{medium} = \\mu_{high}$ vs $H_1$: at least one differs\n      Effect size: Cohen’s d for practical significance\n    \n  \n  Time Series Analysis:\n    \n      ARIMA models for growth prediction based on lagged interaction patterns\n      Granger causality tests to investigate direction of relationships\n    \n  \n  Network Analysis:\n    \n      Graph metrics: in-degree (incoming links), out-degree (outgoing links), centrality measures\n      Community detection: identify clusters of highly connected subreddits\n      Temporal network analysis: how network structure evolves over time\n    \n  \n\n\nProposed Timeline\n\n  Week 1–2: Finalize scraped custom databases, data access and cleaning pipeline; Implement properties as full column for easier manipulation, complete EDA\n  Week 3–4: Baseline modeling/analysis; iterate on features\n  Week 5: Evaluation, robustness checks; draft narrative and visuals\n  Week 6: Refinement and write-up; prepare final notebook and artifacts\n\n\nTeam Organization (towards P3)\n\n  Data engineering and ingestion: Salim Ameziane\n  EDA and visualization: Gauthier Huguelet\n  Modeling/analysis: Achille Pirotais\n  Data Scraping and narrative: Santiago Araya Ramos\n\n\nQuestions for TAs (optional)\n\nRepository Structure\n.\n├─ notebooks/\n│  └─ p2_initial_analysis.ipynb      # single main notebook\n├─ src/\n│  ├─ analysis/\n│  │  └─ eda.py                      # EDA utilities\n│  ├─ data/\n│  │  └─ loader.py                   # data loading &amp; preprocessing\n│  └─ utils/\n│     └─ paths.py                    # path helpers\n├─ data/\n│  ├─ raw/                           # raw input data (not tracked)\n│  └─ processed/                     # processed artifacts (not tracked)\n├─ requirements.txt\n└─ README.md\n\n\nReproducibility &amp; Setup\npython -m venv .venv\n.\\.venv\\Scripts\\activate  # on Windows PowerShell\npip install -r requirements.txt\n\n\nOpen notebooks/p2_initial_analysis.ipynb and run cells in order. The notebook expects data under data/raw/ (see instructions within the notebook or implement download in src/data/loader.py).\n",
    "url": "/"
  },
  
  {
    "title": "The team",
    "excerpt": "\n",
    "content": "\n\n\n\n\n    \n        \n            \n            Santiago Araya Ramos\n            Data Scraping, Data Story development\n        \n    \n    \n        \n            \n            Achille Pirotais\n            Data Modeling and analysis\n        \n    \n    \n        \n            \n            Gauthier Huguelet\n            EDA and visualization\n        \n    \n    \n        \n            \n            Salim Ameziane\n            Data engineering and ingestion\n        \n    \n\n\n",
    "url": "/team/"
  }
  
]

